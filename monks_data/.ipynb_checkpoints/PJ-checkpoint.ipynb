{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision_tree.py\n",
    "# ---------\n",
    "# Licensing Information:  You are free to use or extend these projects for\n",
    "# personal and educational purposes provided that (1) you do not distribute\n",
    "# or publish solutions, (2) you retain this notice, and (3) you provide clear\n",
    "# attribution to UT Dallas, including a link to http://cs.utdallas.edu.\n",
    "#\n",
    "# This file is part of Homework for CS6375: Machine Learning.\n",
    "# Gautam Kunapuli (gautam.kunapuli@utdallas.edu)\n",
    "# Sriraam Natarajan (sriraam.natarajan@utdallas.edu),\n",
    "# Anjum Chida (anjum.chida@utdallas.edu)\n",
    "#\n",
    "#\n",
    "# INSTRUCTIONS:\n",
    "# ------------\n",
    "# 1. This file contains a skeleton for implementing the ID3 algorithm for\n",
    "# Decision Trees. Insert your code into the various functions that have the\n",
    "# comment \"INSERT YOUR CODE HERE\".\n",
    "#\n",
    "# 2. Do NOT modify the classes or functions that have the comment \"DO NOT\n",
    "# MODIFY THIS FUNCTION\".\n",
    "#\n",
    "# 3. Do not modify the function headers for ANY of the functions.\n",
    "#\n",
    "# 4. You may add any other helper functions you feel you may need to print,\n",
    "# visualize, test, or save the data and results. However, you MAY NOT utilize\n",
    "# the package scikit-learn OR ANY OTHER machine learning package in THIS file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/Users/wenxionglu/opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(x):\n",
    "    \"\"\"\n",
    "    Partition the column vector x into subsets indexed by its unique values (v1, ... vk)\n",
    "\n",
    "    Returns a dictionary of the form\n",
    "    { v1: indices of x == v1,\n",
    "      v2: indices of x == v2,\n",
    "      ...\n",
    "      vk: indices of x == vk }, where [v1, ... vk] are all the unique values in the vector z.\n",
    "    \"\"\"\n",
    "\n",
    "    # INSERT YOUR CODE HERE\n",
    "#    raise Exception('Function not yet implemented!')\n",
    "    xsubset = {}\n",
    "    for i in range(len(x)):\n",
    "        if xsubset.get(x[i]) != None:\n",
    "            xsubset[x[i]].append(i)\n",
    "        else:\n",
    "            xsubset[x[i]] = [i]\n",
    "    return xsubset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Compute the entropy of a vector y by considering the counts of the unique values (v1, ... vk), in z\n",
    "\n",
    "    Returns the entropy of z: H(z) = p(z=v1) log2(p(z=v1)) + ... + p(z=vk) log2(p(z=vk))\n",
    "    \"\"\"\n",
    "\n",
    "    # INSERT YOUR CODE HERE\n",
    "#    raise Exception('Function not yet implemented!')\n",
    "    xsubset = partition(y)\n",
    "    e = 0\n",
    "    for i in xsubset.keys():\n",
    "        p = len(xsubset.get(i)) / len(y)\n",
    "        e -= p * np.log2(p)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(x, y):\n",
    "    \"\"\"\n",
    "    Compute the mutual information between a data column (x) and the labels (y). The data column is a single attribute\n",
    "    over all the examples (n x 1). Mutual information is the difference between the entropy BEFORE the split set, and\n",
    "    the weighted-average entropy of EACH possible split.\n",
    "\n",
    "    Returns the mutual information: I(x, y) = H(y) - H(y | x)\n",
    "    \"\"\"\n",
    "\n",
    "    # INSERT YOUR CODE HERE\n",
    "#    raise Exception('Function not yet implemented!')\n",
    "    entro = entropy(y)\n",
    "    xsubset = partition(x)\n",
    "    for i in xsubset.values():\n",
    "        l = []\n",
    "        for j in i:\n",
    "            l.append(y[j])\n",
    "        entro -= len(ls) / len(y) * entropy(ls)\n",
    "    return entro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(x, y, p):\n",
    "    x_true = []\n",
    "    y_true = []\n",
    "    x_false = []\n",
    "    y_false = []\n",
    "    for i in range(len(x)):\n",
    "        if x[i][p[0]] == p[1]:\n",
    "            x_true.append(x[i])\n",
    "            y_true.append(y[i])\n",
    "        else:\n",
    "            x_false.append(x[i])\n",
    "            y_false.append(y[i])\n",
    "    return x_true, y_true, x_false, y_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3(x, y, attribute_value_pairs=None, depth=0, max_depth=5):\n",
    "    \"\"\"\n",
    "    Implements the classical ID3 algorithm given training data (x), training labels (y) and an array of\n",
    "    attribute-value pairs to consider. This is a recursive algorithm that depends on three termination conditions\n",
    "        1. If the entire set of labels (y) is pure (all y = only 0 or only 1), then return that label\n",
    "        2. If the set of attribute-value pairs is empty (there is nothing to split on), then return the most common\n",
    "           value of y (majority label)\n",
    "        3. If the max_depth is reached (pre-pruning bias), then return the most common value of y (majority label)\n",
    "    Otherwise the algorithm selects the next best attribute-value pair using INFORMATION GAIN as the splitting criterion\n",
    "    and partitions the data set based on the values of that attribute before the next recursive call to ID3.\n",
    "\n",
    "    The tree we learn is a BINARY tree, which means that every node has only two branches. The splitting criterion has\n",
    "    to be chosen from among all possible attribute-value pairs. That is, for a problem with two features/attributes x1\n",
    "    (taking values a, b, c) and x2 (taking values d, e), the initial attribute value pair list is a list of all pairs of\n",
    "    attributes with their corresponding values:\n",
    "    [(x1, a),\n",
    "     (x1, b),\n",
    "     (x1, c),\n",
    "     (x2, d),\n",
    "     (x2, e)]\n",
    "     If we select (x2, d) as the best attribute-value pair, then the new decision node becomes: [ (x2 == d)? ] and\n",
    "     the attribute-value pair (x2, d) is removed from the list of attribute_value_pairs.\n",
    "\n",
    "    The tree is stored as a nested dictionary, where each entry is of the form\n",
    "                    (attribute_index, attribute_value, True/False): subtree\n",
    "    * The (attribute_index, attribute_value) determines the splitting criterion of the current node. For example, (4, 2)\n",
    "    indicates that we test if (x4 == 2) at the current node.\n",
    "    * The subtree itself can be nested dictionary, or a single label (leaf node).\n",
    "    * Leaf nodes are (majority) class labels\n",
    "\n",
    "    Returns a decision tree represented as a nested dictionary, for example\n",
    "    {(4, 1, False):\n",
    "        {(0, 1, False):\n",
    "            {(1, 1, False): 1,\n",
    "             (1, 1, True): 0},\n",
    "         (0, 1, True):\n",
    "            {(1, 1, False): 0,\n",
    "             (1, 1, True): 1}},\n",
    "     (4, 1, True): 1}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    tree = {}\n",
    "    if attribute_value_pairs == None:\n",
    "        attribute_value_pairs = []\n",
    "        for row in x:\n",
    "            for i in range(len(row)):\n",
    "                if (i, row[i]) not in attribute_value_pairs:\n",
    "                    attribute_value_pairs.append((i, row[i]))\n",
    "    if (depth == max_depth) or (len(attribute_value_pairs) == 0):\n",
    "        return 1 if 2 * sum(y) >= len(y) else 0\n",
    "    if sum(y) == len(y):\n",
    "        return 1\n",
    "    if sum(y) == 0:\n",
    "        return 0\n",
    "    best_gain = -1\n",
    "    best_pair = ()\n",
    "    x_true = []\n",
    "    y_true = []\n",
    "    x_false = []\n",
    "    y_false = []\n",
    "    for pair in attribute_value_pairs:\n",
    "        e = entropy(y)\n",
    "        curr_x_true, curr_y_true, curr_x_false, curr_y_false = train_test_split(x, y, p)\n",
    "        curr_info_gain = e - entropy(curr_y_true) - entropy(curr_y_false)\n",
    "        if curr_info_gain > best_gain:\n",
    "            best_gain = curr_info_gain\n",
    "            best_pair = pair\n",
    "            x_true = curr_x_true\n",
    "            y_true = curr_y_true\n",
    "            x_false = curr_x_false\n",
    "            y_false = curr_y_false\n",
    "            \n",
    "    attribute_value_pairs.remove(best_pair)\n",
    "    tree.update({(best_pair[0], best_pair[1], True): id3(x_true, y_true, attribute_value_pairs, depth+1, max_depth)})\n",
    "    tree.update({(best_pair[0], best_pair[1], False): id3(x_false, y_false, attribute_value_pairs, depth+1, max_depth)})\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_example(x, tree):\n",
    "    \"\"\"\n",
    "    Predicts the classification label for a single example x using tree by recursively descending the tree until\n",
    "    a label/leaf node is reached.\n",
    "\n",
    "    Returns the predicted label of x according to tree\n",
    "    \"\"\"\n",
    "    if type(tree) == dict:\n",
    "        for i in tree:\n",
    "            if (x[i[0]] == i[1] and i[2] == True) or (x[i[0]] != i[1] and i[2] == False):\n",
    "                return predict_example(x, tree.get(i))\n",
    "    return tree;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the average error between the true labels (y_true) and the predicted labels (y_pred)\n",
    "\n",
    "    Returns the error = (1/n) * sum(y_true != y_pred)\n",
    "    \"\"\"\n",
    "\n",
    "    # INSERT YOUR CODE HERE\n",
    "#    raise Exception('Function not yet implemented!')\n",
    "    total = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] != y_pred[i]:\n",
    "            total += 1\n",
    "    error = total / len(y_true)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(tree, depth=0):\n",
    "    \"\"\"\n",
    "    Pretty prints the decision tree to the console. Use print(tree) to print the raw nested dictionary representation\n",
    "    DO NOT MODIFY THIS FUNCTION!\n",
    "    \"\"\"\n",
    "    if depth == 0:\n",
    "        print('TREE')\n",
    "\n",
    "    for index, split_criterion in enumerate(tree):\n",
    "        sub_trees = tree[split_criterion]\n",
    "\n",
    "        # Print the current node: split criterion\n",
    "        print('|\\t' * depth, end='')\n",
    "        print('+-- [SPLIT: x{0} = {1} {2}]'.format(split_criterion[0], split_criterion[1], split_criterion[2]))\n",
    "\n",
    "        # Print the children\n",
    "        if type(sub_trees) is dict:\n",
    "            pretty_print(sub_trees, depth + 1)\n",
    "        else:\n",
    "            print('|\\t' * (depth + 1), end='')\n",
    "            print('+-- [LABEL = {0}]'.format(sub_trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_dot_file(dot_string, save_file, image_format='png'):\n",
    "    \"\"\"\n",
    "    Uses GraphViz to render a dot file. The dot file can be generated using\n",
    "        * sklearn.tree.export_graphviz()' for decision trees produced by scikit-learn\n",
    "        * to_graphviz() (function is in this file) for decision trees produced by  your code.\n",
    "    DO NOT MODIFY THIS FUNCTION!\n",
    "    \"\"\"\n",
    "    if type(dot_string).__name__ != 'str':\n",
    "        raise TypeError('visualize() requires a string representation of a decision tree.\\nUse tree.export_graphviz()'\n",
    "                        'for decision trees produced by scikit-learn and to_graphviz() for decision trees produced by'\n",
    "                        'your code.\\n')\n",
    "\n",
    "    # Set path to your GraphViz executable here\n",
    "    os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "    graph = graphviz.Source(dot_string)\n",
    "    graph.format = image_format\n",
    "    graph.render(save_file, view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_graphviz(tree, dot_string='', uid=-1, depth=0):\n",
    "    \"\"\"\n",
    "    Converts a tree to DOT format for use with visualize/GraphViz\n",
    "    DO NOT MODIFY THIS FUNCTION!\n",
    "    \"\"\"\n",
    "\n",
    "    uid += 1       # Running index of node ids across recursion\n",
    "    node_id = uid  # Node id of this node\n",
    "\n",
    "    if depth == 0:\n",
    "        dot_string += 'digraph TREE {\\n'\n",
    "\n",
    "    for split_criterion in tree:\n",
    "        sub_trees = tree[split_criterion]\n",
    "        attribute_index = split_criterion[0]\n",
    "        attribute_value = split_criterion[1]\n",
    "        split_decision = split_criterion[2]\n",
    "\n",
    "        if not split_decision:\n",
    "            # Alphabetically, False comes first\n",
    "            dot_string += '    node{0} [label=\"x{1} = {2}?\"];\\n'.format(node_id, attribute_index, attribute_value)\n",
    "\n",
    "        if type(sub_trees) is dict:\n",
    "            if not split_decision:\n",
    "                dot_string, right_child, uid = to_graphviz(sub_trees, dot_string=dot_string, uid=uid, depth=depth + 1)\n",
    "                dot_string += '    node{0} -> node{1} [label=\"False\"];\\n'.format(node_id, right_child)\n",
    "            else:\n",
    "                dot_string, left_child, uid = to_graphviz(sub_trees, dot_string=dot_string, uid=uid, depth=depth + 1)\n",
    "                dot_string += '    node{0} -> node{1} [label=\"True\"];\\n'.format(node_id, left_child)\n",
    "\n",
    "        else:\n",
    "            uid += 1\n",
    "            dot_string += '    node{0} [label=\"y = {1}\"];\\n'.format(uid, sub_trees)\n",
    "            if not split_decision:\n",
    "                dot_string += '    node{0} -> node{1} [label=\"False\"];\\n'.format(node_id, uid)\n",
    "            else:\n",
    "                dot_string += '    node{0} -> node{1} [label=\"True\"];\\n'.format(node_id, uid)\n",
    "\n",
    "    if depth == 0:\n",
    "        dot_string += '}\\n'\n",
    "        return dot_string\n",
    "    else:\n",
    "        return dot_string, node_id, uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_B():\n",
    "    # for each of the three MONK's problem\n",
    "    for monk_set in range(1, 4):\n",
    "        M_trn = np.genfromtxt('./monks-' + str(monk_set) + '.train', missing_values=0, skip_header=0, delimiter=',', dtype=int)\n",
    "        M_tst = np.genfromtxt('./monks-' + str(monk_set) + '.test', missing_values=0, skip_header=0, delimiter=',', dtype=int)\n",
    "        ytrn = M_trn[:, 0]\n",
    "        Xtrn = M_trn[:, 1:]\n",
    "        ytst = M_tst[:, 0]\n",
    "        Xtst = M_tst[:, 1:]\n",
    "        trn_errs = []\n",
    "        tst_errs = []\n",
    "        # for depth 1 to 10\n",
    "        for d in range(1, 11):\n",
    "            # learn decision tree\n",
    "            descision_tree = id3(Xtrn, ytrn, max_depth=d)\n",
    "            # compute errors\n",
    "            y_pred_trn = [predict_example(x, descision_tree) for x in Xtrn]\n",
    "            trn_err = compute_error(ytrn, y_pred_trn)\n",
    "            trn_errs.append(trn_err)\n",
    "            \n",
    "            y_pred_tst = [predict_example(x, descision_tree) for x in Xtst]\n",
    "            tst_err = compute_error(ytst, y_pred_tst)\n",
    "            tst_errs.append(tst_err)\n",
    "        plt.figure()\n",
    "        plt.title('Monk-%i' %monk_set, fontsize=18)\n",
    "        plt.xlabel('Tree Level', fontsize=16)\n",
    "        plt.ylabel('Train_Test_Error', fontsize=16)\n",
    "        plt.plot(np.arange(1,11), trn_errs, marker='o')\n",
    "        plt.plot(np.arange(1,11), tst_errs, marker='x')\n",
    "        plt.legend(['Train_Error', 'Test_Error'])\n",
    "        plt.xticks(np.arange(0,12))\n",
    "        plt.axis([0.5,10.5,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_C():\n",
    "    M_trn = np.genfromtxt('./monks-1.train', missing_values=0, skip_header=0, delimiter=',', dtype=int)\n",
    "    ytrn = M_trn[:, 0]\n",
    "    Xtrn = M_trn[:, 1:]\n",
    "    M_tst = np.genfromtxt('./monks-1.test', missing_values=0, skip_header=0, delimiter=',', dtype=int)\n",
    "    ytst = M_tst[:, 0]\n",
    "    Xtst = M_tst[:, 1:]\n",
    "    for d in [1,3,5]:\n",
    "        decision_tree = id3(Xtrn, ytrn, max_depth=d)\n",
    "        dot_str = to_graphviz(decision_tree)\n",
    "        render_dot_file(dot_str, './question_C_depth=' + str(d))\n",
    "        y_pred = [predict_example(x, decision_tree) for x in Xtst]\n",
    "        print(\"Confusion Matrix for depth = \" + str(d))\n",
    "        print(confusion_matrix(ytst, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_D():\n",
    "    M_trn = np.genfromtxt('./monks-1.train', missing_values=0, skip_header=0, delimiter=',', dtype=int)\n",
    "    ytrn = M_trn[:, 0]\n",
    "    Xtrn = M_trn[:, 1:]\n",
    "    M_tst = np.genfromtxt('./monks-1.test', missing_values=0, skip_header=0, delimiter=',', dtype=int)\n",
    "    ytst = M_tst[:, 0]\n",
    "    Xtst = M_tst[:, 1:]\n",
    "    for d in [1,3,5]:\n",
    "        model = DecisionTreeClassifier(criterion='entropy', max_depth=d)\n",
    "        model.fit(Xtrn, ytrn)\n",
    "        y_pred = model.predict(Xtst)\n",
    "        print(\"Confusion Matrix for depth = \" + str(d))\n",
    "        print(confusion_matrix(ytst, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_E():\n",
    "    M = np.genfromtxt('./tic-tac-toe.data', missing_values=0, skip_header=0, delimiter=',', dtype=str)\n",
    "    X = M[:,:-1]\n",
    "    y = M[:, -1]\n",
    "    Xtrn, Xtst, ytrn, ytst = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    Xtrn_t = Xtrn\n",
    "    Xtst_t = Xtst\n",
    "    le_x = preprocessing.LabelEncoder()\n",
    "    le_x.fit(['b', 'x', 'o'])\n",
    "    for i in range(len(X[0])):\n",
    "        Xtrn_t[:, i] = le_x.transform(Xtrn[:, i])\n",
    "        Xtst_t[:, i] = le_x.transform(Xtst[:, i])\n",
    "    le_y = preprocessing.LabelEncoder()\n",
    "    le_y.fit(['negative', 'positive'])\n",
    "    ytrn_t = le_y.transform(ytrn)\n",
    "    ytst_t = le_y.transform(ytst)\n",
    "    for d in [1,3,5]:\n",
    "        decision_tree = id3(Xtrn, ytrn_t, max_depth=d)\n",
    "        dot_str = to_graphviz(decision_tree)\n",
    "        render_dot_file(dot_str, './question_E_depth=' + str(d))\n",
    "        y_pred = [predict_example(x, decision_tree) for x in Xtst]\n",
    "        print(\"Confusion Matrix for depth = \" + str(d))\n",
    "        print(confusion_matrix(ytst_t, y_pred))\n",
    "        model = DecisionTreeClassifier(criterion='entropy', max_depth=d)\n",
    "        model.fit(Xtrn_t, ytrn_t)\n",
    "        y_pred = model.predict(Xtst_t)\n",
    "        print(\"Confusion Matrix for depth = \" + str(d) + \" (sklearn model)\")\n",
    "        print(confusion_matrix(ytst_t, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-381775a347d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Learn a decision tree of depth 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdecision_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Pretty print it to console\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-725ae38e0590>\u001b[0m in \u001b[0;36mid3\u001b[0;34m(x, y, attribute_value_pairs, depth, max_depth)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattribute_value_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mcurr_x_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_y_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_x_false\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_y_false\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mcurr_info_gain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_y_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_y_false\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurr_info_gain\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_gain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Load the training data\n",
    "    M = np.genfromtxt('./monks-1.train', missing_values=0, skip_header=0, delimiter=',', dtype=int)\n",
    "    ytrn = M[:, 0]\n",
    "    Xtrn = M[:, 1:]\n",
    "\n",
    "    # Load the test data\n",
    "    M = np.genfromtxt('./monks-1.test', missing_values=0, skip_header=0, delimiter=',', dtype=int)\n",
    "    ytst = M[:, 0]\n",
    "    Xtst = M[:, 1:]\n",
    "\n",
    "    # Learn a decision tree of depth 3\n",
    "    decision_tree = id3(Xtrn, ytrn, max_depth=3)\n",
    "\n",
    "    # Pretty print it to console\n",
    "    pretty_print(decision_tree)\n",
    "\n",
    "    # Visualize the tree and save it as a PNG image\n",
    "    dot_str = to_graphviz(decision_tree)\n",
    "    render_dot_file(dot_str, './my_learned_tree')\n",
    "\n",
    "    # Compute the test error\n",
    "    y_pred = [predict_example(x, decision_tree) for x in Xtst]\n",
    "    tst_err = compute_error(ytst, y_pred)\n",
    "\n",
    "    print('Test Error = {0:4.2f}%.'.format(tst_err * 100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
